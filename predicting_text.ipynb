{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "513c5f0f",
   "metadata": {},
   "source": [
    "# Build RNN to generate hunger games sounding text\n",
    "\n",
    "based on tutorial in here : https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "Could not use all features since I have tf version 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62ae9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "13ff1989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18ffb722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"(1) The Hunger Games.txt\", 'rb') as file : \n",
    "    txt_file = file.readlines()\n",
    "type(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b16f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6ccb07b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_all = [str(x)[2:] for x in txt_file][1:]\n",
    "txt_all_long = [x.replace('\\\\r','\\\\s').replace('\\\\n','\\\\s') \n",
    "                for x in txt_all if len(x) > 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d316f1b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_text = ' '.join(txt_all_long)\n",
    "vocab = sorted(set(single_text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "336f99e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size, out_size = 200, 100\n",
    "def split_input_sequence(seq) : \n",
    "    \"\"\"\n",
    "    split sequence into all but last & all but first\n",
    "    \"\"\"\n",
    "    input_text = seq[:-1]\n",
    "    output_text = seq[1:]\n",
    "    return input_text, output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b78c19d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_split = tf.strings.unicode_split(single_text, input_encoding='UTF-8')\n",
    "data = tf.data.Dataset.from_tensor_slices(uc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f5a35aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lookup = {str(x):i for i,x in enumerate(list(vocab))}\n",
    "letter_lookup = {i:str(x) for i,x in enumerate(list(vocab))}\n",
    "uc_np = pd.Series(uc_split.numpy().astype(str)).astype(str).replace(vocab_lookup).values\n",
    "tens = tf.convert_to_tensor(uc_np, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4780553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "60518e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "I\n",
      " \n",
      "w\n",
      "a\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "def translate(n) : \n",
    "    \"\"\"\n",
    "    translate\n",
    "    \"\"\"\n",
    "    return letter_lookup[n]\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(translate(ids.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80896f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[44 55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52\n",
      " 65  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59\n",
      " 51  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68\n",
      " 67  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0\n",
      " 49 68 67  0 53], shape=(101,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(1) : \n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "99880dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : [44 55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52\n",
      " 65  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59\n",
      " 51  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68\n",
      " 67  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0\n",
      " 49 68 67  0]\n",
      "Target: [55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52 65\n",
      "  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59 51\n",
      "  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68 67\n",
      "  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0 49\n",
      " 68 67  0 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 13:29:39.019665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-05 13:29:39.021647: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_sequence)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", (input_example).numpy())\n",
    "    print(\"Target:\", (target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4ee69cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e62d1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 128\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 256\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8e60723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "37838619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 74) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ecfe8994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      multiple                  9472      \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  multiple                  296448    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  19018     \n",
      "=================================================================\n",
      "Total params: 324,938\n",
      "Trainable params: 324,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a8debe",
   "metadata": {},
   "source": [
    "### Untrained model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6c1b96fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([65,  5, 14, 32, 42, 21, 14, 24, 50, 32, 34,  0, 35,  4, 57, 37, 65,\n",
       "       70, 36, 66, 49, 70, 13,  6, 35, 20, 48, 24, 69, 54, 15, 60,  5, 71,\n",
       "        9, 43, 34, 64, 70, 24, 49, 33, 64, 62,  8, 60, 20, 48, 11, 44, 25,\n",
       "        5,  4,  7, 45, 70,  4, 26, 32,  2, 49, 61, 27, 65, 27,  1, 67, 23,\n",
       "       59, 55, 35, 73,  9, 32, 62, 63,  3, 68,  5, 38, 65,  1, 65, 20, 36,\n",
       "       68, 31, 29, 73, 31, 14, 46, 15, 38, 66,  0, 15,  3, 31, 57])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b0380406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_array(arr) : \n",
    "    \"\"\"\n",
    "    translate over & over on an array\n",
    "    \"\"\"\n",
    "    return np.array([translate(a) for a in arr])\n",
    "def translate_array_join(arr) : \n",
    "    \"\"\"\n",
    "    join the text\n",
    "    \"\"\"\n",
    "    return ''.join(translate_array(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "02055e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "  jackets another matter. Stinking and scorched, at least\\s\\s' a foot of the back beyond repair. I cu\n",
      "\n",
      "Next Char Predictions:\n",
      " r,5KU?5CcKM N(jPrwOsbw4-N;aCvg6m,x0VMqwCbLqo/m;a2WD,(.Xw(EK\"bnFrF!tBlhNz0Kop'u,Qr!r;OuJHzJ5Y6Qs 6'Jj\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", translate_array_join(input_example_batch[0].numpy()))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", translate_array_join(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a7ab74",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dc397526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 13:40:15.522739: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-05 13:40:15.878661: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-05 13:40:17.370911: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 10s 60ms/step - loss: 3.2342\n",
      "Epoch 2/20\n",
      "80/80 [==============================] - 5s 61ms/step - loss: 2.8666\n",
      "Epoch 3/20\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 2.7125\n",
      "Epoch 4/20\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 2.7631\n",
      "Epoch 5/20\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 2.7692\n",
      "Epoch 6/20\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 2.6542\n",
      "Epoch 7/20\n",
      "80/80 [==============================] - 5s 58ms/step - loss: 2.5444\n",
      "Epoch 8/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 2.4711\n",
      "Epoch 9/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 2.3555\n",
      "Epoch 10/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 2.2355\n",
      "Epoch 11/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 2.1279\n",
      "Epoch 12/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 2.0696\n",
      "Epoch 13/20\n",
      "80/80 [==============================] - 5s 60ms/step - loss: 2.0305\n",
      "Epoch 14/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.9515\n",
      "Epoch 15/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.8994\n",
      "Epoch 16/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.8394\n",
      "Epoch 17/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.7899\n",
      "Epoch 18/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.7544\n",
      "Epoch 19/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.7304\n",
      "Epoch 20/20\n",
      "80/80 [==============================] - 5s 59ms/step - loss: 1.7040\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "EPOCHS = 20\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "a959be64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 90, 74]), TensorShape([2, 256]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = model(inputs=tf.convert_to_tensor([[0,2,1]*30,[1,2]*45],dtype=tf.int64),states=None,return_state=True)\n",
    "x.shape, y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
