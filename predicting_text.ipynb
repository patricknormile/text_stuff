{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a14641a",
   "metadata": {},
   "source": [
    "# Build RNN to generate hunger games sounding text\n",
    "\n",
    "based on tutorial in here : https://www.tensorflow.org/text/tutorials/text_generation\n",
    "\n",
    "Could not use all features since I have tf version 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62ae9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init Plugin\n",
      "Init Graph Optimizer\n",
      "Init Kernel\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff099fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e49f5ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"(1) The Hunger Games.txt\", 'rb') as file : \n",
    "    txt_file = file.readlines()\n",
    "type(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2acabc5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1071"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "655376a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "487"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_all = [str(x)[2:] for x in txt_file][1:]\n",
    "txt_all_long = [x.replace('\\\\r','\\\\s').replace('\\\\n','\\\\s') \n",
    "                for x in txt_all if len(x) > 49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb9ac1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_text = ' '.join(txt_all_long)\n",
    "vocab = sorted(set(single_text))\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c12e90a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_size, out_size = 200, 100\n",
    "def split_input_sequence(seq) : \n",
    "    \"\"\"\n",
    "    split sequence into all but last & all but first\n",
    "    \"\"\"\n",
    "    input_text = seq[:-1]\n",
    "    output_text = seq[1:]\n",
    "    return input_text, output_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd9037fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "uc_split = tf.strings.unicode_split(single_text, input_encoding='UTF-8')\n",
    "data = tf.data.Dataset.from_tensor_slices(uc_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3e1ff3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_lookup = {str(x):i for i,x in enumerate(list(vocab))}\n",
    "letter_lookup = {i:str(x) for i,x in enumerate(list(vocab))}\n",
    "uc_np = pd.Series(uc_split.numpy().astype(str)).astype(str).replace(vocab_lookup).values\n",
    "tens = tf.convert_to_tensor(uc_np, dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c61e4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "61f642c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W\n",
      "h\n",
      "e\n",
      "n\n",
      " \n",
      "I\n",
      " \n",
      "w\n",
      "a\n",
      "k\n"
     ]
    }
   ],
   "source": [
    "def translate(n) : \n",
    "    \"\"\"\n",
    "    translate\n",
    "    \"\"\"\n",
    "    return letter_lookup[n]\n",
    "for ids in ids_dataset.take(10):\n",
    "    print(translate(ids.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42d4581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[44 55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52\n",
      " 65  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59\n",
      " 51  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68\n",
      " 67  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0\n",
      " 49 68 67  0 53], shape=(101,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "for seq in sequences.take(1) : \n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2ec04fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : [44 55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52\n",
      " 65  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59\n",
      " 51  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68\n",
      " 67  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0\n",
      " 49 68 67  0]\n",
      "Target: [55 52 61  0 30  0 70 48 58 52  0 68 63  5  0 67 55 52  0 62 67 55 52 65\n",
      "  0 66 56 51 52  0 62 53  0 67 55 52  0 49 52 51  0 56 66  0 50 62 59 51\n",
      "  7  0 34 72  0 53 56 61 54 52 65 66  0 66 67 65 52 67 50 55  0 62 68 67\n",
      "  5  0 66 52 52 58 56 61 54  0 37 65 56 60 66  0 70 48 65 60 67 55  0 49\n",
      " 68 67  0 53]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 13:29:39.019665: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-07-05 13:29:39.021647: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "dataset = sequences.map(split_input_sequence)\n",
    "\n",
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", (input_example).numpy())\n",
    "    print(\"Target:\", (target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4e255471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "bef5c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1093584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "521238db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 74) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "d0360fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      multiple                  18944     \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  multiple                  1182720   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  37962     \n",
      "=================================================================\n",
      "Total params: 1,239,626\n",
      "Trainable params: 1,239,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beb98b7",
   "metadata": {},
   "source": [
    "### Untrained model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ad1738e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27, 29,  1, 54, 15, 64,  6, 37, 38,  0, 45, 27, 16, 28,  3, 55, 47,\n",
       "       15,  9, 27, 59, 40, 73, 61, 70, 30, 21,  9,  8, 51, 10, 51, 11, 19,\n",
       "       45, 58, 10, 12, 53,  6, 22, 36, 38, 62, 59, 10, 28,  3, 42, 35, 23,\n",
       "       36, 23, 56, 11, 62, 40,  9, 18, 43, 30, 70, 41, 41, 62, 36, 45, 14,\n",
       "        1, 61,  9, 32, 12, 26,  3, 31, 48, 19, 68,  0, 35, 71, 36, 16,  0,\n",
       "       12, 72,  7, 27, 54, 65, 54, 45, 12, 57, 43, 53, 35, 14, 20])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "a2e506f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_array(arr) : \n",
    "    \"\"\"\n",
    "    translate over & over on an array\n",
    "    \"\"\"\n",
    "    return np.array([translate(a) for a in arr])\n",
    "def translate_array_join(arr) : \n",
    "    \"\"\"\n",
    "    join the text\n",
    "    \"\"\"\n",
    "    return ''.join(translate_array(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "5de1af65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "  unintelligible as they babble out praise. As I glance around, I notice a lot of the other tributes \n",
      "\n",
      "Next Char Predictions:\n",
      " FH!g6q-PQ XF7G'h\\60FlSznwI?0/d1d2:Xk13f-AOQol1G'UNBOBi2oS09VIwTToOX5!n0K3E'Ja:u NxO7 3y.FgrgX3jVfN5;\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", translate_array_join(input_example_batch[0].numpy()))\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", translate_array_join(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8311d5f",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "0aa3256d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-05 15:05:21.982729: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-05 15:05:22.331129: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-07-05 15:05:22.607593: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80/80 [==============================] - 14s 151ms/step - loss: 2.9650\n",
      "Epoch 2/25\n",
      "80/80 [==============================] - 12s 151ms/step - loss: 2.4961\n",
      "Epoch 3/25\n",
      "80/80 [==============================] - 12s 151ms/step - loss: 2.5215\n",
      "Epoch 4/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 2.4581\n",
      "Epoch 5/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 2.2793\n",
      "Epoch 6/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 2.1813\n",
      "Epoch 7/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 2.0795\n",
      "Epoch 8/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 1.9714\n",
      "Epoch 9/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 1.8405\n",
      "Epoch 10/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.7640\n",
      "Epoch 11/25\n",
      "80/80 [==============================] - 12s 151ms/step - loss: 1.6915\n",
      "Epoch 12/25\n",
      "80/80 [==============================] - 13s 156ms/step - loss: 1.6198\n",
      "Epoch 13/25\n",
      "80/80 [==============================] - 12s 152ms/step - loss: 1.5693\n",
      "Epoch 14/25\n",
      "80/80 [==============================] - 13s 153ms/step - loss: 1.5262\n",
      "Epoch 15/25\n",
      "80/80 [==============================] - 12s 149ms/step - loss: 1.4833\n",
      "Epoch 16/25\n",
      "80/80 [==============================] - 13s 154ms/step - loss: 1.4534\n",
      "Epoch 17/25\n",
      "80/80 [==============================] - 13s 153ms/step - loss: 1.4226\n",
      "Epoch 18/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.3971\n",
      "Epoch 19/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.3723\n",
      "Epoch 20/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.3494\n",
      "Epoch 21/25\n",
      "80/80 [==============================] - 12s 152ms/step - loss: 1.3271\n",
      "Epoch 22/25\n",
      "80/80 [==============================] - 13s 153ms/step - loss: 1.3032\n",
      "Epoch 23/25\n",
      "80/80 [==============================] - 13s 153ms/step - loss: 1.2832\n",
      "Epoch 24/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.2625\n",
      "Epoch 25/25\n",
      "80/80 [==============================] - 12s 150ms/step - loss: 1.2474\n"
     ]
    }
   ],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='adam', loss=loss)\n",
    "EPOCHS = 25\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "2464e265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([2, 90, 74]), TensorShape([2, 512]))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y = model(inputs=tf.convert_to_tensor([[0,2,1]*30,[1,2]*45],dtype=tf.int64),states=None,return_state=True)\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "6931842d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\'\""
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_array_join(tf.squeeze(tf.random.categorical(x[:, -1, :], num_samples=1), axis=-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "236a1b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_tens(split) : \n",
    "    \"\"\"\n",
    "    take unicode split array & turn to tensor\n",
    "    \"\"\"\n",
    "    uc_np = pd.DataFrame(split.numpy().astype(str)).astype(str).replace(vocab_lookup).values\n",
    "    tens = tf.convert_to_tensor(uc_np, dtype=tf.int64)\n",
    "    return tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "34cc5097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=string, numpy=\n",
       "array([b'Good morning puppy dog, tided, not atvayled this io my bow alones back, virective from 1ever Cinna st cking and thentsext in his bagglervgueensy I am time to . .s. The sure beitures you can see the Careers forted about of riceat on them until thatfaci wisholights to my boot to the neishat. Because I gnt chinking then you have to drame is, without unatlit eating the paured in my facious berrated the squeeuses. All right? Bur why cakes are said and piss a hay. Handinces. How we war through the mayorr limued intlly of ricking out the cannon quemertifiel gnot tsay of waret of all the trumpets. Of Efthe takes about my head and I bread must position of creeks. It did, seppang her finished and distrestition. Of course, I turn pinkled the morning sincans. Youre my streawn, Im siredy spreads this out of survivated. So she makes it is you, Katniss straight. Im going my head and pussed losides the Careers only as it was knomb? admer? all old solr, I find them on ay anresting my backpy. The lashes Peeta stors and hos in '],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['Good morning puppy dog, '])\n",
    "result = [next_char]\n",
    "for i in range(1000) : \n",
    "    nextchar = tf.strings.unicode_split(next_char, 'UTF-8')\n",
    "    ids = split_to_tens(nextchar)\n",
    "    logits, states = model(inputs=ids, states=states,return_state=True)\n",
    "    predicted_logits = logits[:, -1, :]\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1).numpy()\n",
    "    next_char = translate_array_join(predicted_ids)\n",
    "    result.append(next_char)\n",
    "results = tf.strings.join(result)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1a0aa00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a38f8",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e6708",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
